#!/usr/bin/env python3
import cv2
import numpy as np
import cv2.aruco as aruco
import yaml
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
from std_msgs.msg import String  # For publishing positions as text


class ArucoTracker(Node):
   def __init__(self, camera_matrix_path, perspective_matrix_path):
       """
       ROS2 node that detects ArUco markers and converts them to table-frame coordinates.


       - Loads camera calibration parameters and a precomputed perspective
         transformation matrix from disk.
       - Subscribes to the `/camera` image topic, performs ArUco detection,
         and publishes a visualization image on `/aruco_detection/image`.
       - Publishes detected marker IDs and positions as a debug `String`
         message on `/aruco_detection/positions`.
       - Stores the latest IDs, positions, and frame so other parts of the
         system (e.g., `BlockMover`) can access them.
       """
       super().__init__('aruco_tracker')
      
       # Load camera calibration parameters and perspective transformation matrix
       self.camera_matrix, self.distortion_coefficients = self.load_camera_calibration(camera_matrix_path)
       self.perspective_matrix = np.load(perspective_matrix_path)
      
       # Internal state
       self.latest_ids = None
       self.latest_positions = None
       self.latest_frame = None


       # ROS2 interfaces
       self.bridge = CvBridge()
       self.image_subscription = self.create_subscription(Image, '/image_raw', self.image_callback, 10)
       self.image_publisher = self.create_publisher(Image, '/aruco_detection/image', 10)
       self.position_publisher = self.create_publisher(String, '/aruco_detection/positions', 10)




   @staticmethod
   def crop_frame(image):
       """
       Crop the raw camera frame to focus on the region of interest.


       - Takes an input BGR image and removes margins from the top, bottom,
         left, and right.
       - Returns the cropped image, which can improve detection performance
         by eliminating irrelevant background areas.
       - This helper is available if you want to pre-process images before
         calling `detect_aruco`, but is not used directly in the main pipeline.
       """
       margin_up, margin_down, margin_left, margin_right = 230, 0, 60, 52
       h, w, _ = image.shape
       return image[margin_up:h-margin_down, margin_left:w-margin_right]


   def load_camera_calibration(self, path):
       """
       Load camera intrinsic parameters and distortion coefficients from YAML.


       - `path` points to a YAML file containing `camera_matrix` and
         `distortion_coefficients` as generated by a calibration tool.
       - Returns the 3x3 camera matrix and 1x5 distortion vector as NumPy
         arrays, which are then used by the ArUco pose estimation routines.
       """
       with open(path, 'r') as file:
           yaml_data = yaml.safe_load(file)
       cm = np.array(yaml_data['camera_matrix']['data']).reshape(3, 3)
       distortion_coefficients = np.array(yaml_data['distortion_coefficients']['data']).reshape(1, 5)
       return cm, distortion_coefficients


   def detect_aruco(self, img):
       """
       Detect ArUco markers in an image and draw their outlines and axes.


       - Converts the input BGR image to grayscale and runs OpenCV's ArUco
         detector using the 6x6_250 dictionary.
       - If markers are found, it draws the detected marker borders and
         estimates a pose (rvec, tvec) for each to overlay coordinate axes.
       - Returns a copy of the image with these overlays, along with the
         detected `ids` and `corners` arrays for further processing.
       """
       gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
       aruco_dict = aruco.Dictionary_get(aruco.DICT_6X6_250)
       parameters = aruco.DetectorParameters_create()
       corners, ids, _ = aruco.detectMarkers(gray, aruco_dict, parameters=parameters)
       output = aruco.drawDetectedMarkers(img, corners, ids)


       if ids is not None:
           for i in range(len(ids)):
               rvec, tvec, _ = aruco.estimatePoseSingleMarkers(corners[i], 0.0635, self.camera_matrix, self.distortion_coefficients)
               output = aruco.drawAxis(output, self.camera_matrix, self.distortion_coefficients, rvec, tvec, 0.1)


       return output, ids, corners


   def get_aruco_center(self, img, ids, corners):
       """
       Compute and visualize the 2D pixel centers of each detected ArUco marker.


       - For each entry in `ids` and `corners`, this computes the mean of the
         four corner coordinates to obtain the marker's center in the image.
       - Draws a small circle at each center for visualization.
       - Returns the modified image, the IDs (unchanged), and a list of
         center coordinates corresponding to each ID.
       """
       if ids is None or corners is None:
           return img, [], []


       marker_centers = [(np.nan, np.nan)] * len(ids)
       for i in range(len(ids)):
           corner = corners[i][0, :, :]
           marker_centers[i] = np.mean(corner, axis=0)
           x, y = marker_centers[i]
           img = cv2.circle(img, (int(x), int(y)), 3, (0, 255, 255), -1)
       return img, ids, marker_centers




   def image_frame_to_table_frame(self, img, ids, marker_centers, perspective_matrix):
       """
       Transform marker centers from image pixel coordinates into table frame.


       - `marker_centers` is a list of (u, v) pixel coordinates obtained from
         `get_aruco_center`.
       - `perspective_matrix` is a 3x3 homography that maps points in the
         camera image to the table coordinate system (e.g., in millimeters).
       - Inside the YOUR CODE section you must:
             1. Convert each (u, v) center into homogeneous coordinates.
             2. Apply the perspective transform to obtain table-frame (X, Y).
             3. Normalize by the homogeneous scale factor.
             4. Store the resulting coordinates in `marker_centers_in_table_frame`.
             5. Format and display X/Y values on the image for debugging.
       - Returns both the annotated image and the list of table-frame
         coordinates for use by higher-level logic (e.g., block planning).
       """
       if ids is None or len(ids) == 0:
           return img, []


       marker_centers_in_table_frame = [(np.nan, np.nan)] * len(ids)


       ################################### YOUR CODE STARTS HERE ##################################################
       # EXPECTATION: Map each marker center from image pixels to table frame.
       #
       # A typical implementation will:
       #   1. Loop over every detected marker index `i`.
       #   2. Take the pixel center `(u, v) = marker_centers[i]`.
       #   3. Form a homogeneous coordinate `[u, v, 1]` and multiply by
       #      `perspective_matrix` (a 3x3 homography).
       #   4. Normalize the result so that X = x'/w' and Y = y'/w', where
       #      `[x', y', w']^T = H * [u, v, 1]^T`.
       #   5. Store `(X, Y)` in `marker_centers_in_table_frame[i]`.
       #   6. Convert X and Y (e.g., to millimeters) into strings `value1`,
       #      `value2` for on-screen display.
       #   7. Use `cv2.putText` calls below to overlay the ArUco ID and its
       #      X/Y position in the table frame onto the image.
       #
       # This function should not perform any decision-making about which
       # blocks to move; it only converts and visualizes coordinates.\
      
       for i in range(len(ids)):
           (u, v) = marker_centers[i]




           # Apply perspective transformation
           x = np.array([u, v, 1])
           x_1 = perspective_matrix @ x
           w = x_1[2]


           # Normalize to convert from homogeneous to 2D coordinates
           X = x_1[0]/w
           Y = x_1[1]/w




           # Store the transformed coordinates
           marker_centers_in_table_frame[i] = (X,Y)




           # Format the coordinates as strings for display
           value1 = "" + str(X)
           value2 = "" + str(Y)


           # Display the information on the image
           img = cv2.putText(img, "Aruco ID: " + str(ids[i]), (400, 20 + 90*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2, cv2.LINE_AA, False)
           img = cv2.putText(img, "X (mm): " + value1, (400, 50 + 90*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2, cv2.LINE_AA, False)
           img = cv2.putText(img, "Y (mm): " + value2, (400, 80 + 90*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2, cv2.LINE_AA, False)


       ################################### YOUR CODE ENDS HERE ####################################################
      
       return img, marker_centers_in_table_frame


   def image_callback(self, msg):
       """
       ROS2 image subscription callback for ArUco detection and visualization.


       - Converts the incoming ROS2 `Image` message to an OpenCV BGR frame.
       - Detects ArUco markers, computes their centers, and transforms those
         centers into table-frame coordinates via `image_frame_to_table_frame`.
       - Updates `self.latest_ids`, `self.latest_positions`, and publishes:
             - A visualization image with overlays on `/aruco_detection/image`.
             - A text-formatted list of `(id, position)` pairs on the
               `/aruco_detection/positions` topic for debugging.
       - If no markers are visible, it resets the stored detections to `None`.
       """
       # Convert ROS2 image message to OpenCV format
       frame = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
       self.latest_frame = frame.copy()


       # Process the frame
       frame, ids, corners = self.detect_aruco(frame)
       if ids is not None:
           frame, ids, marker_centers = self.get_aruco_center(frame, ids, corners)
           frame, marker_positions = self.image_frame_to_table_frame(
               frame, ids, marker_centers, self.perspective_matrix
           )


           # Store for others to use
           self.latest_ids = ids
           self.latest_positions = marker_positions


           # Optional: publish positions as a string for debugging
           pos_msg = String()
           pos_msg.data = str(list(zip(ids.flatten().tolist(), marker_positions)))
           self.position_publisher.publish(pos_msg)
       else:
           self.latest_ids = None
           self.latest_positions = None


       # Publish the processed image for visualization
       processed_image_msg = self.bridge.cv2_to_imgmsg(frame, encoding="bgr8")
       self.image_publisher.publish(processed_image_msg)


   def get_latest_detections(self):
       """
       Return the most recent ArUco detections.


       - Provides a tuple `(ids, positions)` where `ids` is the array of
         ArUco IDs and `positions` is the list of corresponding table-frame
         coordinates returned by `image_frame_to_table_frame`.
       """
       return self.latest_ids, self.latest_positions




def main(args=None):
   """
   Standalone entry point to run the ArUco tracker node.


   - Initializes ROS2, creates an `ArucoTracker` with paths to the camera
     calibration and perspective matrix files, and spins it so it can
     process incoming `/camera` images.
   - Use this script when you want to debug marker detection and coordinate
     conversion independently of the full pick-and-place pipeline.
   """
   rclpy.init(args=args)
   camera_matrix_path = '/home/enme480_docker/enme480_ws/src/enme480_project/enme480_project/config/logitech_webcam_640x480.yaml'
   perspective_matrix_path = '/home/enme480_docker/enme480_ws/src/enme480_project/enme480_project/perspective_matrix.npy'
  
   # Initialize and spin the Aruco tracker node
   tracker = ArucoTracker(camera_matrix_path, perspective_matrix_path)
   rclpy.spin(tracker)


   # Cleanup
   tracker.destroy_node()
   rclpy.shutdown()


if __name__ == '__main__':
   main()







